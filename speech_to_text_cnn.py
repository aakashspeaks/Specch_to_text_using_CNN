# -*- coding: utf-8 -*-
"""Speech_to_text_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18KppqsIr4GcyadE6-0vQiBw3AyvW-gOT

**Problem Defination**

Speech Recognition is an important feature in several applications used such as home automation, artificial intelligence, etc. In many cases we need to convert the speech coming from live stream into text.

**Data**

Data has been taken from the : http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz

**Features**

Data Contains a few informational files and a folder of audio files. The audio folder contains subfolders with 1 second clips of voice commands, with the folder name being the label of the audio clip. There are more labels that should be predicted. The labels you will need to predict in Test are yes, no, up, down, left, right, on, off, stop, go. Everything else should be considered either unknown or silence. The folder _background_noise_ contains longer clips of "silence" that you can break up and use as training input.

The files contained in the training audio are not uniquely named across labels, but they are unique if you include the label folder. For example, 00f0204f_nohash_0.wav is found in 14 folders, but that file is a different speech command in each folder.

The files are named so the first element is the subject id of the person who gave the voice command, and the last element indicated repeated commands. Repeated commands are when the subject repeats the same word multiple times. Subject id is not provided for the test data, and you can assume that the majority of commands in the test data were from subjects not seen in train.

You can expect some inconsistencies in the properties of the training data (e.g., length of the audio).
"""

!wget "http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz"

!tar -xvf 'speech_commands_v0.01.tar.gz' -C data/

import os 
import librosa
import IPython.display as ipd
import matplotlib.pyplot as plt
import numpy as np
from scipy.io import wavfile
import warnings
warnings.filterwarnings("ignore")

"""### Data exploration and visualization"""

train_audio_path='data/'
samples,sample_rate = librosa.load(train_audio_path+'yes/0a7c2a8d_nohash_0.wav',sr=16000)
fig = plt.figure(figsize=(14,8))
ax1 = fig.add_subplot(211)
ax1.set_title("raw wav of " +"data/yes/0a7c2a8d_nohash_0.wav")
ax1.set_xlabel("Time")
ax1.set_ylabel("Amplitude")
ax1.plot(np.linspace(0,sample_rate/len(samples),sample_rate),samples);

"""**Sampling rate**

The reason why Iâ€™m doing sample rate conversion is to transform data so that they all have the same shape and easy to be processed with machine learning models. But in real life, there are many more use cases of sample rate conversion.
"""

ipd.Audio(samples,rate=sample_rate)
print(sample_rate)

"""#### Resampling"""

samples = librosa.resample(samples,sample_rate,8000)
ipd.Audio(samples,rate=8000)

"""### Number of recording of each voice"""

labels = os.listdir(train_audio_path)
labels

labels = os.listdir(train_audio_path)
# Find the count of every wav file in different folder
no_of_recording=[]

for label in labels:
  waves = [f for f in os.listdir(train_audio_path+label) if f.endswith('.wav')]
  no_of_recording.append(len(waves))

# plot
fig = plt.figure(figsize=(30,5))
index = np.arange(len(labels))
plt.bar(index,no_of_recording)
plt.xlabel('Commands',fontsize=12)
plt.ylabel('No of recording',fontsize=12)
plt.xticks(index,labels,fontsize=15,rotation=60)
plt.title('Number of recordingd for each command')
plt.show()

labels=["yes","no","up","down","left","right","on","off","stop","go"]

"""### Duration of recordings"""

duration_of_recording=[]

for label in labels:
  waves = [f for f in os.listdir(train_audio_path+ '/' +label) if f.endswith('.wav')]
  for wav in waves:
    sample_rate,samples = wavfile.read(train_audio_path+ '/' +label+ '/' +wav)
    duration_of_recording.append(float(len(samples)/sample_rate))

plt.hist(np.array(duration_of_recording))

train_audio_path = 'data'

all_wave = []
all_label = []
for label in labels:
    print(label)
    waves = [f for f in os.listdir(train_audio_path + '/'+ label) if f.endswith('.wav')]
    for wav in waves:
        samples, sample_rate = librosa.load(train_audio_path + '/' + label + '/' + wav, sr = 16000)
        samples = librosa.resample(samples, sample_rate, 8000)
        if(len(samples)== 8000) : 
            all_wave.append(samples)
            all_label.append(label)

"""#### Preprocess"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y=le.fit_transform(all_label)
classes= list(le.classes_)

from keras.utils import np_utils
y=np_utils.to_categorical(y, num_classes=len(labels))
all_wave = np.array(all_wave).reshape(-1,8000,1)

"""#### Splitiing the data"""

from sklearn.model_selection import train_test_split
x_tr, x_val, y_tr, y_val = train_test_split(np.array(all_wave),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)

"""## Building a model"""

from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D
from keras.models import Model
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras import backend as K
K.clear_session()

inputs = Input(shape=(8000,1))

#First Conv1D layer
conv = Conv1D(8,13, padding='valid', activation='relu', strides=1)(inputs)
conv = MaxPooling1D(3)(conv)
conv = Dropout(0.2)(conv)

#Second Conv1D layer
conv = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(conv)
conv = MaxPooling1D(3)(conv)
conv = Dropout(0.2)(conv)

#Third Conv1D layer
conv = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(conv)
conv = MaxPooling1D(3)(conv)
conv = Dropout(0.2)(conv)

#Fourth Conv1D layer
conv = Conv1D(64, 7, padding='valid', activation='relu', strides=1)(conv)
conv = MaxPooling1D(3)(conv)
conv = Dropout(0.2)(conv)

#Flatten layer
conv = Flatten()(conv)

#Dense Layer 1
conv = Dense(256, activation='relu')(conv)
conv = Dropout(0.2)(conv)

#Dense Layer 2
conv = Dense(128, activation='relu')(conv)
conv = Dropout(0.2)(conv)

outputs = Dense(len(labels), activation='softmax')(conv)

model = Model(inputs, outputs)
model.summary()

model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.00001) 
mc = ModelCheckpoint('best_model.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')

history=model.fit(x_tr, y_tr ,epochs=100, callbacks=[es,mc], batch_size=32, validation_data=(x_val,y_val))

from matplotlib import pyplot 
pyplot.plot(history.history['loss'], label='train') 
pyplot.plot(history.history['val_loss'], label='test') 
pyplot.legend()
pyplot.show()

def predict(audio):
    prob=model.predict(audio.reshape(1,8000,1))
    index=np.argmax(prob[0])
    return classes[index]

import random
index=random.randint(0,len(x_val)-1)
samples=x_val[index].ravel()
print("Audio:",classes[np.argmax(y_val[index])])
ipd.Audio(samples, rate=8000)
print("Text:",predict(samples))